{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-20T13:32:16.141842Z",
     "iopub.status.busy": "2025-09-20T13:32:16.141508Z",
     "iopub.status.idle": "2025-09-20T13:32:25.452722Z",
     "shell.execute_reply": "2025-09-20T13:32:25.451328Z",
     "shell.execute_reply.started": "2025-09-20T13:32:16.141817Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "%pip install nltk scikit-learn numpy python-Levenshtein -q\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T13:49:03.238277Z",
     "iopub.status.busy": "2025-09-20T13:49:03.237796Z",
     "iopub.status.idle": "2025-09-20T13:49:07.844300Z",
     "shell.execute_reply": "2025-09-20T13:49:07.842986Z",
     "shell.execute_reply.started": "2025-09-20T13:49:03.238248Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install rank-bm25 -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T13:49:22.986011Z",
     "iopub.status.busy": "2025-09-20T13:49:22.984493Z",
     "iopub.status.idle": "2025-09-20T13:49:22.995381Z",
     "shell.execute_reply": "2025-09-20T13:49:22.993247Z",
     "shell.execute_reply.started": "2025-09-20T13:49:22.985962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T13:49:23.240709Z",
     "iopub.status.busy": "2025-09-20T13:49:23.240275Z",
     "iopub.status.idle": "2025-09-20T13:49:23.858104Z",
     "shell.execute_reply": "2025-09-20T13:49:23.856741Z",
     "shell.execute_reply.started": "2025-09-20T13:49:23.240683Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8eb7268ca444318bd75e2c35c1bf198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454707e8a3ec4eceb3fd7f5e0b945ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5457b2cba6b54cb7990506553c65e96c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb1959cd8c14c9196d4eee2bcf0c574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/971k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "becaa5199ddb46f4972b1f15a8471922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.01M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c733f2bfcbf4a689d363c6655685f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/946k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7656d1632feb4f45a8a008c41fc80556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0b6e06fab64c7b8bb34f08c5b15eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d9f3e058be40e4a4374d92749f7e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4276b68bf64c4a23ae4c060e890cd0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah total dokumen: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "full_dataset_dict = load_dataset(\"taln-ls2n/inspec\")\n",
    "\n",
    "dataset_to_eval = concatenate_datasets([\n",
    "    full_dataset_dict['train'], \n",
    "    full_dataset_dict['validation'], \n",
    "    full_dataset_dict['test']\n",
    "])\n",
    "\n",
    "print(f\"Jumlah total dokumen: {len(dataset_to_eval)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T13:53:18.740346Z",
     "iopub.status.busy": "2025-09-20T13:53:18.739965Z",
     "iopub.status.idle": "2025-09-20T13:53:18.747598Z",
     "shell.execute_reply": "2025-09-20T13:53:18.746378Z",
     "shell.execute_reply.started": "2025-09-20T13:53:18.740322Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'abstract', 'keyphrases', 'prmu'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_to_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T14:11:43.528593Z",
     "iopub.status.busy": "2025-09-20T14:11:43.528277Z",
     "iopub.status.idle": "2025-09-20T14:11:43.567154Z",
     "shell.execute_reply": "2025-09-20T14:11:43.565401Z",
     "shell.execute_reply.started": "2025-09-20T14:11:43.528570Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "class BM25_RRF_SearchEngine:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = list(dataset)\n",
    "        self.doc_map = {doc['id']: doc for doc in self.dataset}\n",
    "\n",
    "        tokenized_titles = [preprocess(doc.get('title', '')) for doc in self.dataset]\n",
    "        tokenized_keyphrases = [preprocess(' '.join(doc.get('keyphrases', []))) for doc in self.dataset]\n",
    "        tokenized_abstracts = [preprocess(doc.get('abstract', '')) for doc in self.dataset]\n",
    "        \n",
    "        self.vocabulary = set()\n",
    "        for doc_tokens in tokenized_titles:\n",
    "            self.vocabulary.update(doc_tokens)\n",
    "        if any(tokenized_keyphrases):\n",
    "            for doc_tokens in tokenized_keyphrases:\n",
    "                self.vocabulary.update(doc_tokens)\n",
    "        for doc_tokens in tokenized_abstracts:\n",
    "            self.vocabulary.update(doc_tokens)\n",
    "        print(f\"   - Vocabulary dibuat dengan {len(self.vocabulary)} kata unik.\")\n",
    "        \n",
    "        if any(tokenized_titles): self.bm25_title = BM25Okapi(tokenized_titles)\n",
    "        else: self.bm25_title = None\n",
    "            \n",
    "        if any(tokenized_keyphrases): self.bm25_keyphrases = BM25Okapi(tokenized_keyphrases)\n",
    "        else: self.bm25_keyphrases = None\n",
    "            \n",
    "        if any(tokenized_abstracts): self.bm25_abstract = BM25Okapi(tokenized_abstracts)\n",
    "        else: self.bm25_abstract = None\n",
    "            \n",
    "\n",
    "    def _correct_query_word(self, word):\n",
    "        if word in self.vocabulary:\n",
    "            return word\n",
    "        min_dist = float('inf')\n",
    "        corrected_word = word\n",
    "        for vocab_word in self.vocabulary:\n",
    "            dist = levenshtein_distance(word, vocab_word)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                corrected_word = vocab_word\n",
    "        return corrected_word if min_dist <= 3 else word\n",
    "\n",
    "    def search(self, query, top_n=10, k=60):\n",
    "        print(f\"\\n🔎 Mencari untuk query: '{query}'\")\n",
    "        query_tokens = preprocess(query)\n",
    "        \n",
    "        # --- PERUBAHAN LOGIKA PENCETAKAN TYPO DIMULAI DI SINI ---\n",
    "        corrected_query_tokens = []\n",
    "        corrections_made = [] # Untuk menyimpan detail koreksi\n",
    "        typo_found = False # Penanda apakah ada typo\n",
    "\n",
    "        for token in query_tokens:\n",
    "            corrected_word = self._correct_query_word(token)\n",
    "            if corrected_word != token:\n",
    "                typo_found = True\n",
    "                corrections_made.append(f\"'{token}' -> '{corrected_word}'\")\n",
    "            corrected_query_tokens.append(corrected_word)\n",
    "        \n",
    "        # Setelah loop selesai, cetak satu pesan ringkasan JIKA ada typo\n",
    "        if typo_found:\n",
    "            print(f\"    Mendeteksi dan mengoreksi typo: {', '.join(corrections_made)}\")\n",
    "        # --- PERUBAHAN LOGIKA SELESAI DI SINI ---\n",
    "\n",
    "        title_rank_map, keyphrase_rank_map, abstract_rank_map = {}, {}, {}\n",
    "        all_doc_indices = set()\n",
    "        \n",
    "        if self.bm25_title is not None:\n",
    "            title_scores = self.bm25_title.get_scores(corrected_query_tokens)\n",
    "            title_ranks_indices = np.argsort(title_scores)[::-1]\n",
    "            title_rank_map = {doc_idx: rank + 1 for rank, doc_idx in enumerate(title_ranks_indices)}\n",
    "            all_doc_indices.update(title_ranks_indices)\n",
    "\n",
    "        if self.bm25_keyphrases is not None:\n",
    "            keyphrase_scores = self.bm25_keyphrases.get_scores(corrected_query_tokens)\n",
    "            keyphrase_ranks_indices = np.argsort(keyphrase_scores)[::-1]\n",
    "            keyphrase_rank_map = {doc_idx: rank + 1 for rank, doc_idx in enumerate(keyphrase_ranks_indices)}\n",
    "            all_doc_indices.update(keyphrase_ranks_indices)\n",
    "            \n",
    "        if self.bm25_abstract is not None:\n",
    "            abstract_scores = self.bm25_abstract.get_scores(corrected_query_tokens)\n",
    "            abstract_ranks_indices = np.argsort(abstract_scores)[::-1]\n",
    "            abstract_rank_map = {doc_idx: rank + 1 for rank, doc_idx in enumerate(abstract_ranks_indices)}\n",
    "            all_doc_indices.update(abstract_ranks_indices)\n",
    "        \n",
    "        rrf_scores = defaultdict(float)\n",
    "        for doc_idx in all_doc_indices:\n",
    "            score = 0.0\n",
    "            if doc_idx in title_rank_map:\n",
    "                score += 1 / (k + title_rank_map[doc_idx])\n",
    "            if doc_idx in keyphrase_rank_map:\n",
    "                score += 1 / (k + keyphrase_rank_map[doc_idx])\n",
    "            if doc_idx in abstract_rank_map:\n",
    "                score += 1 / (k + abstract_rank_map[doc_idx])\n",
    "            rrf_scores[doc_idx] = score\n",
    "            \n",
    "        sorted_docs = sorted(rrf_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "        \n",
    "        results = []\n",
    "        for doc_idx, score in sorted_docs[:top_n]:\n",
    "            original_doc = self.dataset[doc_idx]\n",
    "            results.append({\n",
    "                'id': original_doc['id'],\n",
    "                'title': original_doc.get('title', 'N/A'),\n",
    "                'score': score,\n",
    "                'abstract': original_doc.get('abstract', 'N/A'),\n",
    "                'keyphrases':  original_doc.get('keyphrases', 'N/A')\n",
    "            })\n",
    "            \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T14:12:54.566464Z",
     "iopub.status.busy": "2025-09-20T14:12:54.566131Z",
     "iopub.status.idle": "2025-09-20T14:13:00.776404Z",
     "shell.execute_reply": "2025-09-20T14:13:00.774623Z",
     "shell.execute_reply.started": "2025-09-20T14:12:54.566433Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Vocabulary dibuat dengan 8909 kata unik.\n",
      "\n",
      "🔎 Mencari untuk query: 'computer Visiom'\n",
      "    Mendeteksi dan mengoreksi typo: 'visiom' -> 'vision'\n",
      "\n",
      "--- Hasil Pencarian ---\n",
      "\n",
      "  ID: 1103, Skor RRF: 0.041959\n",
      "  Title: New age computing [autonomic computing]\n",
      "  Abstract: Autonomic computing (AC), sometimes called self-managed computing, is the name chosen by IBM to describe the company's new initiative aimed at making ...\n",
      "  keyphrases:['new age computing', 'autonomic computing', 'AC', 'self-managed computing', 'IBM initiative', 'computing reliability', 'problem-free computing', 'computer speed', 'computer memory', 'computer crash', 'IT industry initiatives', 'AC requirements', 'AC development', 'AC implementation', 'open standards', 'self-healing computing', 'adaptive algorithms'] \n",
      "\n",
      "  ID: 1785, Skor RRF: 0.041323\n",
      "  Title: The effect of a male-oriented computer gaming culture on careers in the computer industry\n",
      "  Abstract: If careers in the computer industry were viewed, it would be evident that there is a conspicuous gender gap between the number of male and female empl...\n",
      "  keyphrases:['computer games', 'careers', 'computer industry', 'gender gap', 'female employees', 'spatial learning', 'cognitive processing', 'marketing', 'male stereotypes', 'computer science degree', 'computer literacy'] \n",
      "\n",
      "  ID: 1733, Skor RRF: 0.037306\n",
      "  Title: Computing grid unlocks research\n",
      "  Abstract: Under the UK government's spending review in 2000 the Office of Science and Technology was allocated Pounds 98m to establish a three year e-science re...\n",
      "  keyphrases:['e-science', 'collaboration', 'computing resources', 'software', 'open source prototypes', 'middleware', 'UK programme', 'grid computing', 'scientific research'] \n",
      "\n",
      "  ID: 875, Skor RRF: 0.036905\n",
      "  Title: Women of color in computing\n",
      "  Abstract: It is well known that there is a need to increase the number of women in the area of computing, that is in computer science and computer engineering. ...\n",
      "  keyphrases:['women of color', 'computer science', 'computer engineering', 'higher education', 'ethnic minority', 'society', 'gender issues'] \n",
      "\n",
      "  ID: 214, Skor RRF: 0.036765\n",
      "  Title: Evolution of the high-end computing market in the USA\n",
      "  Abstract: This paper focuses on the technological change in the high-end computing market. The discussion combines historical analysis with strategic analysis t...\n",
      "  keyphrases:['USA', 'historical analysis', 'strategic analysis', 'computer industry', 'government research', 'development spending', 'technology strategy', 'new product innovation', 'competition', 'low-end personal computer market', 'parallel computing architectures', 'high-end computing market evolution', 'supercomputing'] \n"
     ]
    }
   ],
   "source": [
    "engine_rrf = BM25_RRF_SearchEngine(dataset_to_eval)\n",
    "\n",
    "query = \"computer Visiom\"\n",
    "search_results = engine_rrf.search(query, top_n=5, k=60)\n",
    "\n",
    "print(\"\\n--- Hasil Pencarian ---\")\n",
    "for doc in search_results:\n",
    "    print(f\"\\n  ID: {doc['id']}, Skor RRF: {doc['score']:.6f}\")\n",
    "    print(f\"  Title: {doc['title']}\")\n",
    "    print(f\"  Abstract: {doc['abstract'][:150]}...\")\n",
    "    print(f\"  keyphrases:{doc['keyphrases']} \")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Information Retrieval Search Engine with Clustering\n",
    "# Simple function-based approach\n",
    "\n",
    "%pip install nltk scikit-learn numpy python-Levenshtein rank-bm25 datasets matplotlib seaborn -q\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 2000 documents\n",
      "Sample document keys: ['id', 'title', 'abstract', 'keyphrases', 'prmu']\n",
      "Sample title: Towards a NMR implementation of a quantum lattice gas algorithm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "full_dataset_dict = load_dataset(\"taln-ls2n/inspec\")\n",
    "dataset_to_eval = concatenate_datasets([\n",
    "    full_dataset_dict['train'], \n",
    "    full_dataset_dict['validation'], \n",
    "    full_dataset_dict['test']\n",
    "])\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset_to_eval)} documents\")\n",
    "print(f\"Sample document keys: {list(dataset_to_eval[0].keys())}\")\n",
    "print(f\"Sample title: {dataset_to_eval[0]['title']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Computer Vision and Machine Learning algorithms\n",
      "Processed: ['comput', 'vision', 'machin', 'learn', 'algorithm']\n"
     ]
    }
   ],
   "source": [
    "# Function 1: Text Preprocessing\n",
    "def preprocess(text):\n",
    "    \"\"\"Preprocess text: tokenize, lowercase, remove stopwords, stem\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Test preprocessing\n",
    "test_text = \"Computer Vision and Machine Learning algorithms\"\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Processed: {preprocess(test_text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 8909\n",
      "Sample words: ['symmetr', 'spectral', 'connot', 'thyristor', 'septal', 'vagu', 'dictionari', 'atm', 'perfect', 'biotechnolog']\n"
     ]
    }
   ],
   "source": [
    "# Function 2: Build Vocabulary\n",
    "def build_vocabulary(dataset):\n",
    "    \"\"\"Build vocabulary from all documents\"\"\"\n",
    "    vocabulary = set()\n",
    "    for doc in dataset:\n",
    "        title_tokens = preprocess(doc.get('title', ''))\n",
    "        abstract_tokens = preprocess(doc.get('abstract', ''))\n",
    "        keyphrase_tokens = preprocess(' '.join(doc.get('keyphrases', [])))\n",
    "        vocabulary.update(title_tokens + abstract_tokens + keyphrase_tokens)\n",
    "    return vocabulary\n",
    "\n",
    "# Build vocabulary\n",
    "vocabulary = build_vocabulary(dataset_to_eval)\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "print(f\"Sample words: {list(vocabulary)[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: computer visiom\n",
      "Corrected: comput vision\n"
     ]
    }
   ],
   "source": [
    "# Function 3: Query Correction\n",
    "def correct_query_word(word, vocabulary):\n",
    "    \"\"\"Correct typos in query using Levenshtein distance\"\"\"\n",
    "    if word in vocabulary:\n",
    "        return word\n",
    "    min_dist = float('inf')\n",
    "    corrected_word = word\n",
    "    for vocab_word in vocabulary:\n",
    "        dist = levenshtein_distance(word, vocab_word)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            corrected_word = vocab_word\n",
    "    return corrected_word if min_dist <= 3 else word\n",
    "\n",
    "# Test query correction\n",
    "test_query = \"computer visiom\"\n",
    "corrected = [correct_query_word(word, vocabulary) for word in preprocess(test_query)]\n",
    "print(f\"Original: {test_query}\")\n",
    "print(f\"Corrected: {' '.join(corrected)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 setup completed\n"
     ]
    }
   ],
   "source": [
    "# Function 4: Setup BM25\n",
    "def setup_bm25(dataset):\n",
    "    \"\"\"Setup BM25 for title, abstract, and keyphrases\"\"\"\n",
    "    tokenized_titles = [preprocess(doc.get('title', '')) for doc in dataset]\n",
    "    tokenized_abstracts = [preprocess(doc.get('abstract', '')) for doc in dataset]\n",
    "    tokenized_keyphrases = [preprocess(' '.join(doc.get('keyphrases', []))) for doc in dataset]\n",
    "    \n",
    "    bm25_title = BM25Okapi(tokenized_titles) if any(tokenized_titles) else None\n",
    "    bm25_abstract = BM25Okapi(tokenized_abstracts) if any(tokenized_abstracts) else None\n",
    "    bm25_keyphrases = BM25Okapi(tokenized_keyphrases) if any(tokenized_keyphrases) else None\n",
    "    \n",
    "    return bm25_title, bm25_abstract, bm25_keyphrases\n",
    "\n",
    "# Setup BM25\n",
    "bm25_title, bm25_abstract, bm25_keyphrases = setup_bm25(dataset_to_eval)\n",
    "print(\"BM25 setup completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up clustering...\n",
      "Clustering completed: 10 clusters\n"
     ]
    }
   ],
   "source": [
    "# Function 5: Setup Clustering\n",
    "def setup_clustering(dataset, n_clusters=10):\n",
    "    \"\"\"Setup document clustering using K-Means\"\"\"\n",
    "    print(\"Setting up clustering...\")\n",
    "    \n",
    "    # Combine all text for clustering\n",
    "    combined_texts = []\n",
    "    for doc in dataset:\n",
    "        title = doc.get('title', '')\n",
    "        abstract = doc.get('abstract', '')\n",
    "        keyphrases = ' '.join(doc.get('keyphrases', []))\n",
    "        combined_text = f\"{title} {abstract} {keyphrases}\"\n",
    "        combined_texts.append(combined_text)\n",
    "    \n",
    "    # Create TF-IDF vectors\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english', ngram_range=(1, 2))\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(combined_texts)\n",
    "    \n",
    "    # Perform K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=min(n_clusters, len(dataset)), random_state=42, n_init=10)\n",
    "    doc_clusters = kmeans.fit_predict(tfidf_matrix)\n",
    "    \n",
    "    print(f\"Clustering completed: {len(set(doc_clusters))} clusters\")\n",
    "    return tfidf_vectorizer, kmeans, doc_clusters\n",
    "\n",
    "# Setup clustering\n",
    "tfidf_vectorizer, kmeans, doc_clusters = setup_clustering(dataset_to_eval, n_clusters=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: machine learning\n",
      "Relevant clusters: [1, 9, 3]\n"
     ]
    }
   ],
   "source": [
    "# Function 6: Find Relevant Clusters\n",
    "def find_relevant_clusters(query, tfidf_vectorizer, kmeans):\n",
    "    \"\"\"Find clusters most relevant to the query\"\"\"\n",
    "    query_vector = tfidf_vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(query_vector, kmeans.cluster_centers_)[0]\n",
    "    \n",
    "    # Get top clusters above threshold\n",
    "    threshold = 0.1\n",
    "    relevant_clusters = [i for i, sim in enumerate(similarities) if sim > threshold]\n",
    "    \n",
    "    # If no clusters above threshold, return top 3\n",
    "    if not relevant_clusters:\n",
    "        relevant_clusters = np.argsort(similarities)[-3:].tolist()\n",
    "    \n",
    "    return relevant_clusters\n",
    "\n",
    "# Test cluster finding\n",
    "test_query = \"machine learning\"\n",
    "relevant_clusters = find_relevant_clusters(test_query, tfidf_vectorizer, kmeans)\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Relevant clusters: {relevant_clusters}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for: 'computer vision'\n",
      "Found 1 relevant clusters\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Wrong key type: '728' of type '<class 'numpy.int64'>'. Expected one of int, slice, range, str or Iterable.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Test search\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m results = \u001b[43msearch_with_clustering\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcomputer vision\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_to_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbm25_title\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbm25_abstract\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mbm25_keyphrases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfidf_vectorizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkmeans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_clusters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTop 3 results:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results[:\u001b[32m3\u001b[39m], \u001b[32m1\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36msearch_with_clustering\u001b[39m\u001b[34m(query, dataset, bm25_title, bm25_abstract, bm25_keyphrases, tfidf_vectorizer, kmeans, doc_clusters, vocabulary, top_n, k)\u001b[39m\n\u001b[32m     55\u001b[39m results = []\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc_idx, score \u001b[38;5;129;01min\u001b[39;00m sorted_docs[:top_n]:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     doc = \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     58\u001b[39m     results.append({\n\u001b[32m     59\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m: doc[\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     60\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m: doc.get(\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mN/A\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcluster_id\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(doc_clusters[doc_idx])\n\u001b[32m     65\u001b[39m     })\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\arrow_dataset.py:2803\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2801\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[32m   2802\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2803\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\arrow_dataset.py:2787\u001b[39m, in \u001b[36mDataset._getitem\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m format_kwargs = format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m   2786\u001b[39m formatter = get_formatter(format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m pa_subtable = \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_indices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_indices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2788\u001b[39m formatted_output = format_table(\n\u001b[32m   2789\u001b[39m     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\n\u001b[32m   2790\u001b[39m )\n\u001b[32m   2791\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\formatting\\formatting.py:578\u001b[39m, in \u001b[36mquery_table\u001b[39m\u001b[34m(table, key, indices)\u001b[39m\n\u001b[32m    576\u001b[39m \u001b[38;5;66;03m# Check if key is valid\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mslice\u001b[39m, \u001b[38;5;28mrange\u001b[39m, \u001b[38;5;28mstr\u001b[39m, Iterable)):\n\u001b[32m--> \u001b[39m\u001b[32m578\u001b[39m     \u001b[43m_raise_bad_key_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    580\u001b[39m     _check_valid_column_key(key, table.column_names)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\formatting\\formatting.py:45\u001b[39m, in \u001b[36m_raise_bad_key_type\u001b[39m\u001b[34m(key)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_raise_bad_key_type\u001b[39m(key: Any):\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m     46\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWrong key type: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m of type \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(key)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Expected one of int, slice, range, str or Iterable.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     47\u001b[39m     )\n",
      "\u001b[31mTypeError\u001b[39m: Wrong key type: '728' of type '<class 'numpy.int64'>'. Expected one of int, slice, range, str or Iterable."
     ]
    }
   ],
   "source": [
    "# Function 7: Search with Clustering\n",
    "def search_with_clustering(query, dataset, bm25_title, bm25_abstract, bm25_keyphrases, \n",
    "                          tfidf_vectorizer, kmeans, doc_clusters, vocabulary, top_n=10, k=60):\n",
    "    \"\"\"Search with clustering boost\"\"\"\n",
    "    print(f\"Searching for: '{query}'\")\n",
    "    \n",
    "    # Preprocess and correct query\n",
    "    query_tokens = preprocess(query)\n",
    "    corrected_tokens = [correct_query_word(word, vocabulary) for word in query_tokens]\n",
    "    \n",
    "    # Find relevant clusters\n",
    "    relevant_clusters = find_relevant_clusters(query, tfidf_vectorizer, kmeans)\n",
    "    print(f\"Found {len(relevant_clusters)} relevant clusters\")\n",
    "    \n",
    "    # Get BM25 scores\n",
    "    title_rank_map, abstract_rank_map, keyphrase_rank_map = {}, {}, {}\n",
    "    all_doc_indices = set()\n",
    "    \n",
    "    if bm25_title is not None:\n",
    "        title_scores = bm25_title.get_scores(corrected_tokens)\n",
    "        title_ranks = np.argsort(title_scores)[::-1]\n",
    "        title_rank_map = {doc_idx: rank + 1 for rank, doc_idx in enumerate(title_ranks)}\n",
    "        all_doc_indices.update(title_ranks)\n",
    "    \n",
    "    if bm25_abstract is not None:\n",
    "        abstract_scores = bm25_abstract.get_scores(corrected_tokens)\n",
    "        abstract_ranks = np.argsort(abstract_scores)[::-1]\n",
    "        abstract_rank_map = {doc_idx: rank + 1 for rank, doc_idx in enumerate(abstract_ranks)}\n",
    "        all_doc_indices.update(abstract_ranks)\n",
    "    \n",
    "    if bm25_keyphrases is not None:\n",
    "        keyphrase_scores = bm25_keyphrases.get_scores(corrected_tokens)\n",
    "        keyphrase_ranks = np.argsort(keyphrase_scores)[::-1]\n",
    "        keyphrase_rank_map = {doc_idx: rank + 1 for rank, doc_idx in enumerate(keyphrase_ranks)}\n",
    "        all_doc_indices.update(keyphrase_ranks)\n",
    "    \n",
    "    # Calculate RRF scores with clustering boost\n",
    "    rrf_scores = defaultdict(float)\n",
    "    for doc_idx in all_doc_indices:\n",
    "        cluster_boost = 1.2 if doc_clusters[doc_idx] in relevant_clusters else 1.0\n",
    "        \n",
    "        score = 0.0\n",
    "        if doc_idx in title_rank_map:\n",
    "            score += 1 / (k + title_rank_map[doc_idx])\n",
    "        if doc_idx in abstract_rank_map:\n",
    "            score += 1 / (k + abstract_rank_map[doc_idx])\n",
    "        if doc_idx in keyphrase_rank_map:\n",
    "            score += 1 / (k + keyphrase_rank_map[doc_idx])\n",
    "        \n",
    "        rrf_scores[doc_idx] = score * cluster_boost\n",
    "    \n",
    "    # Sort and return results\n",
    "    sorted_docs = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    results = []\n",
    "    for doc_idx, score in sorted_docs[:top_n]:\n",
    "        doc = dataset[doc_idx]\n",
    "        results.append({\n",
    "            'id': doc['id'],\n",
    "            'title': doc.get('title', 'N/A'),\n",
    "            'score': score,\n",
    "            'abstract': doc.get('abstract', 'N/A'),\n",
    "            'keyphrases': doc.get('keyphrases', []),\n",
    "            'cluster_id': int(doc_clusters[doc_idx])\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test search\n",
    "results = search_with_clustering(\"computer vision\", dataset_to_eval, bm25_title, bm25_abstract, \n",
    "                                bm25_keyphrases, tfidf_vectorizer, kmeans, doc_clusters, vocabulary)\n",
    "print(f\"\\nTop 3 results:\")\n",
    "for i, doc in enumerate(results[:3], 1):\n",
    "    print(f\"{i}. {doc['title']} (Score: {doc['score']:.4f}, Cluster: {doc['cluster_id']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 8: Visualize Clusters\n",
    "def visualize_clusters(doc_clusters, dataset):\n",
    "    \"\"\"Visualize cluster distribution\"\"\"\n",
    "    cluster_counts = {}\n",
    "    for cluster_id in doc_clusters:\n",
    "        cluster_counts[cluster_id] = cluster_counts.get(cluster_id, 0) + 1\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Bar chart\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(cluster_counts.keys(), cluster_counts.values(), color='skyblue', alpha=0.7)\n",
    "    plt.xlabel('Cluster ID')\n",
    "    plt.ylabel('Number of Documents')\n",
    "    plt.title('Cluster Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Pie chart\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.pie(cluster_counts.values(), labels=[f'Cluster {i}' for i in cluster_counts.keys()], \n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "    plt.title('Cluster Size Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Cluster distribution:\")\n",
    "    for cluster_id, count in sorted(cluster_counts.items()):\n",
    "        percentage = (count / len(dataset)) * 100\n",
    "        print(f\"  Cluster {cluster_id}: {count} documents ({percentage:.1f}%)\")\n",
    "\n",
    "# Visualize clusters\n",
    "visualize_clusters(doc_clusters, dataset_to_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 9: Test Different Queries\n",
    "def test_queries(queries, dataset, bm25_title, bm25_abstract, bm25_keyphrases, \n",
    "                tfidf_vectorizer, kmeans, doc_clusters, vocabulary):\n",
    "    \"\"\"Test multiple queries\"\"\"\n",
    "    for i, query in enumerate(queries, 1):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"TEST {i}: {query}\")\n",
    "        print('='*50)\n",
    "        \n",
    "        results = search_with_clustering(query, dataset, bm25_title, bm25_abstract, \n",
    "                                       bm25_keyphrases, tfidf_vectorizer, kmeans, \n",
    "                                       doc_clusters, vocabulary, top_n=3)\n",
    "        \n",
    "        for j, doc in enumerate(results, 1):\n",
    "            print(f\"\\n{j}. {doc['title']}\")\n",
    "            print(f\"   Score: {doc['score']:.4f}\")\n",
    "            print(f\"   Cluster: {doc['cluster_id']}\")\n",
    "            print(f\"   Abstract: {doc['abstract'][:100]}...\")\n",
    "\n",
    "# Test multiple queries\n",
    "test_queries = [\n",
    "    \"machine learning algorithms\",\n",
    "    \"computer vision applications\",\n",
    "    \"natural language processing\",\n",
    "    \"information retrieval systems\",\n",
    "    \"artificial intelligence\"\n",
    "]\n",
    "\n",
    "test_queries(test_queries, dataset_to_eval, bm25_title, bm25_abstract, bm25_keyphrases,\n",
    "            tfidf_vectorizer, kmeans, doc_clusters, vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 10: Compare with/without Clustering\n",
    "def compare_search_methods(query, dataset, bm25_title, bm25_abstract, bm25_keyphrases, \n",
    "                          tfidf_vectorizer, kmeans, doc_clusters, vocabulary):\n",
    "    \"\"\"Compare search with and without clustering\"\"\"\n",
    "    print(f\"COMPARISON FOR: '{query}'\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Search with clustering\n",
    "    print(\"\\nüîç WITH CLUSTERING:\")\n",
    "    results_with = search_with_clustering(query, dataset, bm25_title, bm25_abstract, \n",
    "                                        bm25_keyphrases, tfidf_vectorizer, kmeans, \n",
    "                                        doc_clusters, vocabulary, top_n=3)\n",
    "    \n",
    "    # Search without clustering (set all cluster boosts to 1.0)\n",
    "    print(f\"\\nüîç WITHOUT CLUSTERING:\")\n",
    "    print(f\"Searching for: '{query}'\")\n",
    "    \n",
    "    query_tokens = preprocess(query)\n",
    "    corrected_tokens = [correct_query_word(word, vocabulary) for word in query_tokens]\n",
    "    \n",
    "    # Get BM25 scores (same as before)\n",
    "    title_rank_map, abstract_rank_map, keyphrase_rank_map = {}, {}, {}\n",
    "    all_doc_indices = set()\n",
    "    \n",
    "    if bm25_title is not None:\n",
    "        title_scores = bm25_title.get_scores(corrected_tokens)\n",
    "        title_ranks = np.argsort(title_scores)[::-1]\n",
    "        title_rank_map = {doc_idx: rank + 1 for rank, doc_idx in enumerate(title_ranks)}\n",
    "        all_doc_indices.update(title_ranks)\n",
    "    \n",
    "    if bm25_abstract is not None:\n",
    "        abstract_scores = bm25_abstract.get_scores(corrected_tokens)\n",
    "        abstract_ranks = np.argsort(abstract_scores)[::-1]\n",
    "        abstract_rank_map = {doc_idx: rank + 1 for rank, doc_idx in enumerate(abstract_ranks)}\n",
    "        all_doc_indices.update(abstract_ranks)\n",
    "    \n",
    "    if bm25_keyphrases is not None:\n",
    "        keyphrase_scores = bm25_keyphrases.get_scores(corrected_tokens)\n",
    "        keyphrase_ranks = np.argsort(keyphrase_scores)[::-1]\n",
    "        keyphrase_rank_map = {doc_idx: rank + 1 for rank, doc_idx in enumerate(keyphrase_ranks)}\n",
    "        all_doc_indices.update(keyphrase_ranks)\n",
    "    \n",
    "    # Calculate RRF scores WITHOUT clustering boost\n",
    "    rrf_scores = defaultdict(float)\n",
    "    for doc_idx in all_doc_indices:\n",
    "        score = 0.0\n",
    "        if doc_idx in title_rank_map:\n",
    "            score += 1 / (60 + title_rank_map[doc_idx])\n",
    "        if doc_idx in abstract_rank_map:\n",
    "            score += 1 / (60 + abstract_rank_map[doc_idx])\n",
    "        if doc_idx in keyphrase_rank_map:\n",
    "            score += 1 / (60 + keyphrase_rank_map[doc_idx])\n",
    "        rrf_scores[doc_idx] = score\n",
    "    \n",
    "    # Sort and return results\n",
    "    sorted_docs = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    results_without = []\n",
    "    for doc_idx, score in sorted_docs[:3]:\n",
    "        doc = dataset[doc_idx]\n",
    "        results_without.append({\n",
    "            'id': doc['id'],\n",
    "            'title': doc.get('title', 'N/A'),\n",
    "            'score': score,\n",
    "            'abstract': doc.get('abstract', 'N/A'),\n",
    "            'keyphrases': doc.get('keyphrases', []),\n",
    "            'cluster_id': int(doc_clusters[doc_idx])\n",
    "        })\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nüìä WITH CLUSTERING:\")\n",
    "    for i, doc in enumerate(results_with, 1):\n",
    "        print(f\"{i}. {doc['title']} (Score: {doc['score']:.4f}, Cluster: {doc['cluster_id']})\")\n",
    "    \n",
    "    print(\"\\nüìä WITHOUT CLUSTERING:\")\n",
    "    for i, doc in enumerate(results_without, 1):\n",
    "        print(f\"{i}. {doc['title']} (Score: {doc['score']:.4f}, Cluster: {doc['cluster_id']})\")\n",
    "    \n",
    "    # Compare\n",
    "    with_ids = [doc['id'] for doc in results_with]\n",
    "    without_ids = [doc['id'] for doc in results_without]\n",
    "    common = set(with_ids) & set(without_ids)\n",
    "    \n",
    "    print(f\"\\nüîÑ COMPARISON:\")\n",
    "    print(f\"Common documents: {len(common)}/3\")\n",
    "    print(f\"Clustering boost applied: {len([d for d in results_with if d['cluster_id'] in find_relevant_clusters(query, tfidf_vectorizer, kmeans)])} docs\")\n",
    "\n",
    "# Test comparison\n",
    "compare_search_methods(\"computer vision\", dataset_to_eval, bm25_title, bm25_abstract, \n",
    "                     bm25_keyphrases, tfidf_vectorizer, kmeans, doc_clusters, vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-20T13:32:16.141842Z",
     "iopub.status.busy": "2025-09-20T13:32:16.141508Z",
     "iopub.status.idle": "2025-09-20T13:32:25.452722Z",
     "shell.execute_reply": "2025-09-20T13:32:25.451328Z",
     "shell.execute_reply.started": "2025-09-20T13:32:16.141817Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "%pip install nltk scikit-learn numpy python-Levenshtein -q\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T13:49:03.238277Z",
     "iopub.status.busy": "2025-09-20T13:49:03.237796Z",
     "iopub.status.idle": "2025-09-20T13:49:07.844300Z",
     "shell.execute_reply": "2025-09-20T13:49:07.842986Z",
     "shell.execute_reply.started": "2025-09-20T13:49:03.238248Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install rank-bm25 -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T13:49:22.986011Z",
     "iopub.status.busy": "2025-09-20T13:49:22.984493Z",
     "iopub.status.idle": "2025-09-20T13:49:22.995381Z",
     "shell.execute_reply": "2025-09-20T13:49:22.993247Z",
     "shell.execute_reply.started": "2025-09-20T13:49:22.985962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T13:49:23.240709Z",
     "iopub.status.busy": "2025-09-20T13:49:23.240275Z",
     "iopub.status.idle": "2025-09-20T13:49:23.858104Z",
     "shell.execute_reply": "2025-09-20T13:49:23.856741Z",
     "shell.execute_reply.started": "2025-09-20T13:49:23.240683Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah total dokumen: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "full_dataset_dict = load_dataset(\"taln-ls2n/inspec\")\n",
    "\n",
    "dataset_to_eval = concatenate_datasets([\n",
    "    full_dataset_dict['train'], \n",
    "    full_dataset_dict['validation'], \n",
    "    full_dataset_dict['test']\n",
    "])\n",
    "\n",
    "print(f\"Jumlah total dokumen: {len(dataset_to_eval)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T13:53:18.740346Z",
     "iopub.status.busy": "2025-09-20T13:53:18.739965Z",
     "iopub.status.idle": "2025-09-20T13:53:18.747598Z",
     "shell.execute_reply": "2025-09-20T13:53:18.746378Z",
     "shell.execute_reply.started": "2025-09-20T13:53:18.740322Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'abstract', 'keyphrases', 'prmu'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_to_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T14:11:43.528593Z",
     "iopub.status.busy": "2025-09-20T14:11:43.528277Z",
     "iopub.status.idle": "2025-09-20T14:11:43.567154Z",
     "shell.execute_reply": "2025-09-20T14:11:43.565401Z",
     "shell.execute_reply.started": "2025-09-20T14:11:43.528570Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "class BM25_RRF_SearchEngine:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = list(dataset)\n",
    "        self.doc_map = {doc['id']: doc for doc in self.dataset}\n",
    "\n",
    "        tokenized_titles = [preprocess(doc.get('title', '')) for doc in self.dataset]\n",
    "        tokenized_keyphrases = [preprocess(' '.join(doc.get('keyphrases', []))) for doc in self.dataset]\n",
    "        tokenized_abstracts = [preprocess(doc.get('abstract', '')) for doc in self.dataset]\n",
    "        \n",
    "        self.vocabulary = set()\n",
    "        for doc_tokens in tokenized_titles:\n",
    "            self.vocabulary.update(doc_tokens)\n",
    "        if any(tokenized_keyphrases):\n",
    "            for doc_tokens in tokenized_keyphrases:\n",
    "                self.vocabulary.update(doc_tokens)\n",
    "        for doc_tokens in tokenized_abstracts:\n",
    "            self.vocabulary.update(doc_tokens)\n",
    "        print(f\"   - Vocabulary dibuat dengan {len(self.vocabulary)} kata unik.\")\n",
    "        \n",
    "        if any(tokenized_titles): self.bm25_title = BM25Okapi(tokenized_titles)\n",
    "        else: self.bm25_title = None\n",
    "            \n",
    "        if any(tokenized_keyphrases): self.bm25_keyphrases = BM25Okapi(tokenized_keyphrases)\n",
    "        else: self.bm25_keyphrases = None\n",
    "            \n",
    "        if any(tokenized_abstracts): self.bm25_abstract = BM25Okapi(tokenized_abstracts)\n",
    "        else: self.bm25_abstract = None\n",
    "            \n",
    "\n",
    "    def _correct_query_word(self, word):\n",
    "        if word in self.vocabulary:\n",
    "            return word\n",
    "        min_dist = float('inf')\n",
    "        corrected_word = word\n",
    "        for vocab_word in self.vocabulary:\n",
    "            dist = levenshtein_distance(word, vocab_word)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                corrected_word = vocab_word\n",
    "        return corrected_word if min_dist <= 3 else word\n",
    "\n",
    "    def search(self, query, top_n=10, k=60):\n",
    "        print(f\"\\nüîé Mencari untuk query: '{query}'\")\n",
    "        query_tokens = preprocess(query)\n",
    "        \n",
    "        # --- PERUBAHAN LOGIKA PENCETAKAN TYPO DIMULAI DI SINI ---\n",
    "        corrected_query_tokens = []\n",
    "        corrections_made = [] # Untuk menyimpan detail koreksi\n",
    "        typo_found = False # Penanda apakah ada typo\n",
    "\n",
    "        for token in query_tokens:\n",
    "            corrected_word = self._correct_query_word(token)\n",
    "            if corrected_word != token:\n",
    "                typo_found = True\n",
    "                corrections_made.append(f\"'{token}' -> '{corrected_word}'\")\n",
    "            corrected_query_tokens.append(corrected_word)\n",
    "        \n",
    "        # Setelah loop selesai, cetak satu pesan ringkasan JIKA ada typo\n",
    "        if typo_found:\n",
    "            print(f\"    Mendeteksi dan mengoreksi typo: {', '.join(corrections_made)}\")\n",
    "        # --- PERUBAHAN LOGIKA SELESAI DI SINI ---\n",
    "\n",
    "        title_rank_map, keyphrase_rank_map, abstract_rank_map = {}, {}, {}\n",
    "        all_doc_indices = set()\n",
    "        \n",
    "        if self.bm25_title is not None:\n",
    "            title_scores = self.bm25_title.get_scores(corrected_query_tokens)\n",
    "            title_ranks_indices = np.argsort(title_scores)[::-1]\n",
    "            title_rank_map = {doc_idx: rank + 1 for rank, doc_idx in enumerate(title_ranks_indices)}\n",
    "            all_doc_indices.update(title_ranks_indices)\n",
    "\n",
    "        if self.bm25_keyphrases is not None:\n",
    "            keyphrase_scores = self.bm25_keyphrases.get_scores(corrected_query_tokens)\n",
    "            keyphrase_ranks_indices = np.argsort(keyphrase_scores)[::-1]\n",
    "            keyphrase_rank_map = {doc_idx: rank + 1 for rank, doc_idx in enumerate(keyphrase_ranks_indices)}\n",
    "            all_doc_indices.update(keyphrase_ranks_indices)\n",
    "            \n",
    "        if self.bm25_abstract is not None:\n",
    "            abstract_scores = self.bm25_abstract.get_scores(corrected_query_tokens)\n",
    "            abstract_ranks_indices = np.argsort(abstract_scores)[::-1]\n",
    "            abstract_rank_map = {doc_idx: rank + 1 for rank, doc_idx in enumerate(abstract_ranks_indices)}\n",
    "            all_doc_indices.update(abstract_ranks_indices)\n",
    "        \n",
    "        rrf_scores = defaultdict(float)\n",
    "        for doc_idx in all_doc_indices:\n",
    "            score = 0.0\n",
    "            if doc_idx in title_rank_map:\n",
    "                score += 1 / (k + title_rank_map[doc_idx])\n",
    "            if doc_idx in keyphrase_rank_map:\n",
    "                score += 1 / (k + keyphrase_rank_map[doc_idx])\n",
    "            if doc_idx in abstract_rank_map:\n",
    "                score += 1 / (k + abstract_rank_map[doc_idx])\n",
    "            rrf_scores[doc_idx] = score\n",
    "            \n",
    "        sorted_docs = sorted(rrf_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "        \n",
    "        results = []\n",
    "        for doc_idx, score in sorted_docs[:top_n]:\n",
    "            original_doc = self.dataset[doc_idx]\n",
    "            results.append({\n",
    "                'id': original_doc['id'],\n",
    "                'title': original_doc.get('title', 'N/A'),\n",
    "                'score': score,\n",
    "                'abstract': original_doc.get('abstract', 'N/A'),\n",
    "                'keyphrases':  original_doc.get('keyphrases', 'N/A')\n",
    "            })\n",
    "            \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T14:12:54.566464Z",
     "iopub.status.busy": "2025-09-20T14:12:54.566131Z",
     "iopub.status.idle": "2025-09-20T14:13:00.776404Z",
     "shell.execute_reply": "2025-09-20T14:13:00.774623Z",
     "shell.execute_reply.started": "2025-09-20T14:12:54.566433Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Vocabulary dibuat dengan 8909 kata unik.\n",
      "\n",
      "üîé Mencari untuk query: 'computer Visiom'\n",
      "    Mendeteksi dan mengoreksi typo: 'visiom' -> 'vision'\n",
      "\n",
      "--- Hasil Pencarian ---\n",
      "\n",
      "  ID: 1103, Skor RRF: 0.041959\n",
      "  Title: New age computing [autonomic computing]\n",
      "  Abstract: Autonomic computing (AC), sometimes called self-managed computing, is the name chosen by IBM to describe the company's new initiative aimed at making ...\n",
      "  keyphrases:['new age computing', 'autonomic computing', 'AC', 'self-managed computing', 'IBM initiative', 'computing reliability', 'problem-free computing', 'computer speed', 'computer memory', 'computer crash', 'IT industry initiatives', 'AC requirements', 'AC development', 'AC implementation', 'open standards', 'self-healing computing', 'adaptive algorithms'] \n",
      "\n",
      "  ID: 1785, Skor RRF: 0.041323\n",
      "  Title: The effect of a male-oriented computer gaming culture on careers in the computer industry\n",
      "  Abstract: If careers in the computer industry were viewed, it would be evident that there is a conspicuous gender gap between the number of male and female empl...\n",
      "  keyphrases:['computer games', 'careers', 'computer industry', 'gender gap', 'female employees', 'spatial learning', 'cognitive processing', 'marketing', 'male stereotypes', 'computer science degree', 'computer literacy'] \n",
      "\n",
      "  ID: 1733, Skor RRF: 0.037306\n",
      "  Title: Computing grid unlocks research\n",
      "  Abstract: Under the UK government's spending review in 2000 the Office of Science and Technology was allocated Pounds 98m to establish a three year e-science re...\n",
      "  keyphrases:['e-science', 'collaboration', 'computing resources', 'software', 'open source prototypes', 'middleware', 'UK programme', 'grid computing', 'scientific research'] \n",
      "\n",
      "  ID: 875, Skor RRF: 0.036905\n",
      "  Title: Women of color in computing\n",
      "  Abstract: It is well known that there is a need to increase the number of women in the area of computing, that is in computer science and computer engineering. ...\n",
      "  keyphrases:['women of color', 'computer science', 'computer engineering', 'higher education', 'ethnic minority', 'society', 'gender issues'] \n",
      "\n",
      "  ID: 214, Skor RRF: 0.036765\n",
      "  Title: Evolution of the high-end computing market in the USA\n",
      "  Abstract: This paper focuses on the technological change in the high-end computing market. The discussion combines historical analysis with strategic analysis t...\n",
      "  keyphrases:['USA', 'historical analysis', 'strategic analysis', 'computer industry', 'government research', 'development spending', 'technology strategy', 'new product innovation', 'competition', 'low-end personal computer market', 'parallel computing architectures', 'high-end computing market evolution', 'supercomputing'] \n"
     ]
    }
   ],
   "source": [
    "engine_rrf = BM25_RRF_SearchEngine(dataset_to_eval)\n",
    "\n",
    "query = \"computer Visiom\"\n",
    "search_results = engine_rrf.search(query, top_n=5, k=60)\n",
    "\n",
    "print(\"\\n--- Hasil Pencarian ---\")\n",
    "for doc in search_results:\n",
    "    print(f\"\\n  ID: {doc['id']}, Skor RRF: {doc['score']:.6f}\")\n",
    "    print(f\"  Title: {doc['title']}\")\n",
    "    print(f\"  Abstract: {doc['abstract'][:150]}...\")\n",
    "    print(f\"  keyphrases:{doc['keyphrases']} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Setting up document clustering...\n",
      "‚úÖ Clustering completed: 10 clusters created\n",
      "\n",
      "üìä Cluster Distribution:\n",
      "  Cluster 0: 524 documents\n",
      "  Cluster 1: 181 documents\n",
      "  Cluster 2: 98 documents\n",
      "  Cluster 3: 529 documents\n",
      "  Cluster 4: 219 documents\n",
      "  Cluster 5: 43 documents\n",
      "  Cluster 6: 84 documents\n",
      "  Cluster 7: 77 documents\n",
      "  Cluster 8: 42 documents\n",
      "  Cluster 9: 203 documents\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clustering Implementation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def setup_document_clustering(dataset, n_clusters=10):\n",
    "    \"\"\"Setup document clustering using K-Means\"\"\"\n",
    "    print(\"üîÑ Setting up document clustering...\")\n",
    "    \n",
    "    # Combine all text for clustering\n",
    "    combined_texts = []\n",
    "    for doc in dataset:\n",
    "        title = doc.get('title', '')\n",
    "        abstract = doc.get('abstract', '')\n",
    "        keyphrases = ' '.join(doc.get('keyphrases', []))\n",
    "        combined_text = f\"{title} {abstract} {keyphrases}\"\n",
    "        combined_texts.append(combined_text)\n",
    "    \n",
    "    # Create TF-IDF vectors for clustering\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_features=1000,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2)\n",
    "    )\n",
    "    \n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(combined_texts)\n",
    "    \n",
    "    # Perform K-Means clustering\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=min(n_clusters, len(dataset)),\n",
    "        random_state=42,\n",
    "        n_init=10\n",
    "    )\n",
    "    \n",
    "    doc_clusters = kmeans.fit_predict(tfidf_matrix)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    \n",
    "    print(f\"‚úÖ Clustering completed: {len(set(doc_clusters))} clusters created\")\n",
    "    \n",
    "    return tfidf_vectorizer, kmeans, doc_clusters, cluster_centers\n",
    "\n",
    "# Setup clustering\n",
    "tfidf_vectorizer, kmeans, doc_clusters, cluster_centers = setup_document_clustering(dataset_to_eval, n_clusters=10)\n",
    "\n",
    "# Analyze cluster distribution\n",
    "cluster_counts = {}\n",
    "for cluster_id in doc_clusters:\n",
    "    cluster_counts[cluster_id] = cluster_counts.get(cluster_id, 0) + 1\n",
    "\n",
    "print(\"\\nüìä Cluster Distribution:\")\n",
    "for cluster_id, count in sorted(cluster_counts.items()):\n",
    "    print(f\"  Cluster {cluster_id}: {count} documents\")\n",
    "\n",
    "# Visualize cluster distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(cluster_counts.keys(), cluster_counts.values())\n",
    "plt.xlabel('Cluster ID')\n",
    "plt.ylabel('Number of Documents')\n",
    "plt.title('Document Distribution Across Clusters')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Search Engine with Clustering\n",
    "class BM25_RRF_Clustered_SearchEngine:\n",
    "    def __init__(self, dataset, n_clusters=10):\n",
    "        self.dataset = list(dataset)\n",
    "        self.doc_map = {doc['id']: doc for doc in self.dataset}\n",
    "        self.n_clusters = n_clusters\n",
    "\n",
    "        tokenized_titles = [preprocess(doc.get('title', '')) for doc in self.dataset]\n",
    "        tokenized_keyphrases = [preprocess(' '.join(doc.get('keyphrases', []))) for doc in self.dataset]\n",
    "        tokenized_abstracts = [preprocess(doc.get('abstract', '')) for doc in self.dataset]\n",
    "        \n",
    "        self.vocabulary = set()\n",
    "        for doc_tokens in tokenized_titles:\n",
    "            self.vocabulary.update(doc_tokens)\n",
    "        if any(tokenized_keyphrases):\n",
    "            for doc_tokens in tokenized_keyphrases:\n",
    "                self.vocabulary.update(doc_tokens)\n",
    "        for doc_tokens in tokenized_abstracts:\n",
    "            self.vocabulary.update(doc_tokens)\n",
    "        \n",
    "        if any(tokenized_titles): \n",
    "            self.bm25_title = BM25Okapi(tokenized_titles)\n",
    "        else: \n",
    "            self.bm25_title = None\n",
    "            \n",
    "        if any(tokenized_keyphrases): \n",
    "            self.bm25_keyphrases = BM25Okapi(tokenized_keyphrases)\n",
    "        else: \n",
    "            self.bm25_keyphrases = None\n",
    "            \n",
    "        if any(tokenized_abstracts): \n",
    "            self.bm25_abstract = BM25Okapi(tokenized_abstracts)\n",
    "        else: \n",
    "            self.bm25_abstract = None\n",
    "        \n",
    "        # Setup clustering\n",
    "        self._setup_clustering()\n",
    "    \n",
    "    def _setup_clustering(self):\n",
    "        \"\"\"Setup document clustering using K-Means\"\"\"\n",
    "        print(\"üîÑ Setting up document clustering...\")\n",
    "        \n",
    "        # Combine all text for clustering\n",
    "        combined_texts = []\n",
    "        for doc in self.dataset:\n",
    "            title = doc.get('title', '')\n",
    "            abstract = doc.get('abstract', '')\n",
    "            keyphrases = ' '.join(doc.get('keyphrases', []))\n",
    "            combined_text = f\"{title} {abstract} {keyphrases}\"\n",
    "            combined_texts.append(combined_text)\n",
    "        \n",
    "        # Create TF-IDF vectors for clustering\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_features=1000,\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "        \n",
    "        tfidf_matrix = self.tfidf_vectorizer.fit_transform(combined_texts)\n",
    "        \n",
    "        # Perform K-Means clustering\n",
    "        self.kmeans = KMeans(\n",
    "            n_clusters=min(self.n_clusters, len(self.dataset)),\n",
    "            random_state=42,\n",
    "            n_init=10\n",
    "        )\n",
    "        \n",
    "        self.doc_clusters = self.kmeans.fit_predict(tfidf_matrix)\n",
    "        self.cluster_centers = self.kmeans.cluster_centers_\n",
    "        \n",
    "        # Create cluster to documents mapping\n",
    "        self.cluster_to_docs = defaultdict(list)\n",
    "        for i, cluster_id in enumerate(self.doc_clusters):\n",
    "            self.cluster_to_docs[cluster_id].append(i)\n",
    "        \n",
    "        print(f\"‚úÖ Clustering completed: {len(set(self.doc_clusters))} clusters created\")\n",
    "    \n",
    "    def get_cluster_info(self, doc_id):\n",
    "        \"\"\"Get cluster information for a document\"\"\"\n",
    "        for i, doc in enumerate(self.dataset):\n",
    "            if str(doc['id']) == str(doc_id):\n",
    "                cluster_id = self.doc_clusters[i]\n",
    "                cluster_docs = self.cluster_to_docs[cluster_id]\n",
    "                return {\n",
    "                    'cluster_id': int(cluster_id),\n",
    "                    'cluster_size': len(cluster_docs),\n",
    "                    'similar_docs': [self.dataset[j]['id'] for j in cluster_docs[:5]]  # Top 5 similar docs\n",
    "                }\n",
    "        return None\n",
    "\n",
    "    def _correct_query_word(self, word):\n",
    "        if word in self.vocabulary:\n",
    "            return word\n",
    "        min_dist = float('inf')\n",
    "        corrected_word = word\n",
    "        for vocab_word in self.vocabulary:\n",
    "            dist = levenshtein_distance(word, vocab_word)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                corrected_word = vocab_word\n",
    "        return corrected_word if min_dist <= 3 else word\n",
    "\n",
    "    def search(self, query, top_n=10, k=60, use_clustering=True):\n",
    "        query_tokens = preprocess(query)\n",
    "        \n",
    "        corrected_query_tokens = []\n",
    "        corrections_made = []\n",
    "        typo_found = False\n",
    "\n",
    "        for token in query_tokens:\n",
    "            corrected_word = self._correct_query_word(token)\n",
    "            if corrected_word != token:\n",
    "                typo_found = True\n",
    "                corrections_made.append(f\"'{token}' -> '{corrected_word}'\")\n",
    "            corrected_query_tokens.append(corrected_word)\n",
    "\n",
    "        # If using clustering, find relevant clusters first\n",
    "        if use_clustering:\n",
    "            relevant_clusters = self._find_relevant_clusters(query)\n",
    "            print(f\"üéØ Found {len(relevant_clusters)} relevant clusters\")\n",
    "        else:\n",
    "            relevant_clusters = None\n",
    "\n",
    "        title_rank_map, keyphrase_rank_map, abstract_rank_map = {}, {}, {}\n",
    "        all_doc_indices = set()\n",
    "        \n",
    "        if self.bm25_title is not None:\n",
    "            title_scores = self.bm25_title.get_scores(corrected_query_tokens)\n",
    "            title_ranks_indices = np.argsort(title_scores)[::-1]\n",
    "            title_rank_map = {doc_idx: rank + 1 for rank, doc_idx in enumerate(title_ranks_indices)}\n",
    "            all_doc_indices.update(title_ranks_indices)\n",
    "\n",
    "        if self.bm25_keyphrases is not None:\n",
    "            keyphrase_scores = self.bm25_keyphrases.get_scores(corrected_query_tokens)\n",
    "            keyphrase_ranks_indices = np.argsort(keyphrase_scores)[::-1]\n",
    "            keyphrase_rank_map = {doc_idx: rank + 1 for rank, doc_idx in enumerate(keyphrase_ranks_indices)}\n",
    "            all_doc_indices.update(keyphrase_ranks_indices)\n",
    "            \n",
    "        if self.bm25_abstract is not None:\n",
    "            abstract_scores = self.bm25_abstract.get_scores(corrected_query_tokens)\n",
    "            abstract_ranks_indices = np.argsort(abstract_scores)[::-1]\n",
    "            abstract_rank_map = {doc_idx: rank + 1 for rank, doc_idx in enumerate(abstract_ranks_indices)}\n",
    "            all_doc_indices.update(abstract_ranks_indices)\n",
    "        \n",
    "        rrf_scores = defaultdict(float)\n",
    "        for doc_idx in all_doc_indices:\n",
    "            # Apply clustering boost if document is in relevant clusters\n",
    "            cluster_boost = 1.0\n",
    "            if use_clustering and relevant_clusters is not None:\n",
    "                doc_cluster = self.doc_clusters[doc_idx]\n",
    "                if doc_cluster in relevant_clusters:\n",
    "                    cluster_boost = 1.2  # 20% boost for relevant cluster documents\n",
    "            \n",
    "            score = 0.0\n",
    "            if doc_idx in title_rank_map:\n",
    "                score += 1 / (k + title_rank_map[doc_idx])\n",
    "            if doc_idx in keyphrase_rank_map:\n",
    "                score += 1 / (k + keyphrase_rank_map[doc_idx])\n",
    "            if doc_idx in abstract_rank_map:\n",
    "                score += 1 / (k + abstract_rank_map[doc_idx])\n",
    "            \n",
    "            rrf_scores[doc_idx] = score * cluster_boost\n",
    "            \n",
    "        sorted_docs = sorted(rrf_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "        \n",
    "        results = []\n",
    "        for doc_idx, score in sorted_docs[:top_n]:\n",
    "            original_doc = self.dataset[doc_idx]\n",
    "            cluster_info = self.get_cluster_info(original_doc['id'])\n",
    "            results.append({\n",
    "                'id': original_doc['id'],\n",
    "                'title': original_doc.get('title', 'N/A'),\n",
    "                'score': score,\n",
    "                'abstract': original_doc.get('abstract', 'N/A'),\n",
    "                'keyphrases': original_doc.get('keyphrases', 'N/A'),\n",
    "                'corrections': corrections_made if typo_found else [],\n",
    "                'cluster_id': cluster_info['cluster_id'] if cluster_info else None,\n",
    "                'cluster_size': cluster_info['cluster_size'] if cluster_info else None\n",
    "            })\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def _find_relevant_clusters(self, query):\n",
    "        \"\"\"Find clusters most relevant to the query\"\"\"\n",
    "        # Transform query to TF-IDF vector\n",
    "        query_vector = self.tfidf_vectorizer.transform([query])\n",
    "        \n",
    "        # Calculate similarity between query and cluster centers\n",
    "        similarities = cosine_similarity(query_vector, self.cluster_centers)[0]\n",
    "        \n",
    "        # Get top clusters (above threshold)\n",
    "        threshold = 0.1\n",
    "        relevant_clusters = []\n",
    "        for i, sim in enumerate(similarities):\n",
    "            if sim > threshold:\n",
    "                relevant_clusters.append(i)\n",
    "        \n",
    "        # If no clusters above threshold, return top 3\n",
    "        if not relevant_clusters:\n",
    "            top_indices = np.argsort(similarities)[-3:]\n",
    "            relevant_clusters = top_indices.tolist()\n",
    "        \n",
    "        return relevant_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Setting up document clustering...\n",
      "‚úÖ Clustering completed: 10 clusters created\n",
      "\n",
      "üîç Testing search with clustering for: 'computer vision machine learning'\n",
      "üéØ Found 2 relevant clusters\n",
      "\n",
      "--- Results with Clustering ---\n",
      "\n",
      "  ID: 284, Score: 0.054323\n",
      "  Title: Project-based learning: teachers learning and using high-tech to preserve Cajun culture\n",
      "  Cluster: 3 (size: 529)\n",
      "  Abstract: Using project-based learning pedagogy in EdTc 658 Advances in Educational Technology, the author has trained inservice teachers in Southwestern Louisi...\n",
      "\n",
      "  ID: 1286, Score: 0.048332\n",
      "  Title: Self-describing Turing machines\n",
      "  Cluster: 3 (size: 529)\n",
      "  Abstract: After a sketchy historical account on the question of self-describeness and self-reproduction, and after discussing the definition of suitable encodin...\n",
      "\n",
      "  ID: 963, Score: 0.046927\n",
      "  Title: A computational model of learned avoidance behavior in a one-way avoidance experiment\n",
      "  Cluster: 0 (size: 524)\n",
      "  Abstract: We present a computational model of learned avoidance behavior in a one-way avoidance experiment. Our model employs the reinforcement learning paradig...\n",
      "\n",
      "  ID: 1785, Score: 0.044148\n",
      "  Title: The effect of a male-oriented computer gaming culture on careers in the computer industry\n",
      "  Cluster: 8 (size: 42)\n",
      "  Abstract: If careers in the computer industry were viewed, it would be evident that there is a conspicuous gender gap between the number of male and female empl...\n",
      "\n",
      "  ID: 1015, Score: 0.043531\n",
      "  Title: Scalable techniques from nonparametric statistics for real time robot learning\n",
      "  Cluster: 3 (size: 529)\n",
      "  Abstract: Locally weighted learning (LWL) is a class of techniques from nonparametric statistics that provides useful representations and training algorithms fo...\n"
     ]
    }
   ],
   "source": [
    "# Test the enhanced search engine with clustering\n",
    "engine_clustered = BM25_RRF_Clustered_SearchEngine(dataset_to_eval, n_clusters=10)\n",
    "\n",
    "query = \"computer vision machine learning\"\n",
    "print(f\"\\nüîç Testing search with clustering for: '{query}'\")\n",
    "\n",
    "# Search with clustering enabled\n",
    "search_results_clustered = engine_clustered.search(query, top_n=5, k=60, use_clustering=True)\n",
    "\n",
    "print(\"\\n--- Results ---\")\n",
    "for doc in search_results_clustered:\n",
    "    print(f\"\\n  ID: {doc['id']}, Score: {doc['score']:.6f}\")\n",
    "    print(f\"  Title: {doc['title']}\")\n",
    "    print(f\"  Cluster: {doc['cluster_id']} (size: {doc['cluster_size']})\")\n",
    "    print(f\"  Abstract: {doc['abstract'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster-then-Search Engine - Complete Implementation\n",
    "\n",
    "This implementation follows the detailed specifications:\n",
    "- Phase 1: Offline initialization (clustering, BM25 models, vocabulary)\n",
    "- Phase 2: Online search with cluster filtering and RRF\n",
    "\n",
    "Key features:\n",
    "1. Typo correction with edit distance <= 2\n",
    "2. Cluster filtering using cosine similarity\n",
    "3. BM25 search ONLY within winning cluster\n",
    "4. Reciprocal Rank Fusion (RRF) for combining rankings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete Cluster-then-Search Engine Implementation\n",
    "Author: AI Assistant\n",
    "Date: 2024\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from rank_bm25 import BM25Okapi\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "class ClusterThenSearchEngine:\n",
    "    \"\"\"\n",
    "    Search Engine dengan Cluster-then-Search Architecture\n",
    "    \n",
    "    Fase 1 (Offline): Menginisialisasi clustering, BM25 models, dan vocabulary\n",
    "    Fase 2 (Online): Mencari dengan cluster filtering dan RRF\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, n_clusters=10):\n",
    "        \"\"\"\n",
    "        Phase 1: Offline Initialization\n",
    "        \n",
    "        Args:\n",
    "            dataset: Dataset dari Hugging Face\n",
    "            n_clusters: Jumlah klaster untuk K-Means\n",
    "        \"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"üîÑ PHASE 1: OFFLINE INITIALIZATION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        self.dataset = list(dataset)\n",
    "        self.n_docs = len(self.dataset)\n",
    "        self.n_clusters = min(n_clusters, self.n_docs)\n",
    "        self.doc_ids = [doc['id'] for doc in self.dataset]\n",
    "        \n",
    "        # Step 1: Load and preprocess dataset\n",
    "        print(\"\\nüìù Step 1: Loading and preprocessing dataset...\")\n",
    "        self._prepare_documents()\n",
    "        \n",
    "        # Step 2: Build vocabulary for typo correction\n",
    "        print(\"\\nüìö Step 2: Building vocabulary for typo correction...\")\n",
    "        self._build_vocabulary()\n",
    "        \n",
    "        # Step 3: TF-IDF Vectorization\n",
    "        print(\"\\nüìä Step 3: Creating TF-IDF vectors...\")\n",
    "        self._create_tfidf_vectors()\n",
    "        \n",
    "        # Step 4: K-Means Clustering\n",
    "        print(\"\\nüéØ Step 4: Performing K-Means clustering...\")\n",
    "        self._perform_clustering()\n",
    "        \n",
    "        # Step 5: Initialize BM25 models\n",
    "        print(\"\\nüîç Step 5: Initializing BM25 models...\")\n",
    "        self._initialize_bm25_models()\n",
    "        \n",
    "        print(\"\\n‚úÖ Phase 1 completed successfully!\")\n",
    "        print(f\"   - Total documents: {self.n_docs}\")\n",
    "        print(f\"   - Vocabulary size: {len(self.vocabulary)}\")\n",
    "        print(f\"   - Number of clusters: {len(set(self.doc_cluster_labels))}\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    def _prepare_documents(self):\n",
    "        \"\"\"Prepare documents: preprocess title, abstract, and keyphrases separately\"\"\"\n",
    "        self.processed_titles = []\n",
    "        self.processed_abstracts = []\n",
    "        self.processed_keyphrases = []\n",
    "        self.combined_texts = []  # For clustering\n",
    "        \n",
    "        for doc in self.dataset:\n",
    "            # Preprocess each field separately\n",
    "            title = doc.get('title', '')\n",
    "            abstract = doc.get('abstract', '')\n",
    "            keyphrases = ' '.join(doc.get('keyphrases', []))\n",
    "            \n",
    "            self.processed_titles.append(preprocess(title))\n",
    "            self.processed_abstracts.append(preprocess(abstract))\n",
    "            self.processed_keyphrases.append(preprocess(keyphrases))\n",
    "            \n",
    "            # Combine all text for clustering\n",
    "            combined = ' '.join([title, abstract, keyphrases])\n",
    "            self.combined_texts.append(combined)\n",
    "        \n",
    "        print(f\"   ‚úì Processed {len(self.processed_titles)} documents\")\n",
    "    \n",
    "    def _build_vocabulary(self):\n",
    "        \"\"\"Build vocabulary from entire corpus for typo correction\"\"\"\n",
    "        self.vocabulary = set()\n",
    "        for tokens in self.processed_titles:\n",
    "            self.vocabulary.update(tokens)\n",
    "        for tokens in self.processed_abstracts:\n",
    "            self.vocabulary.update(tokens)\n",
    "        for tokens in self.processed_keyphrases:\n",
    "            self.vocabulary.update(tokens)\n",
    "        \n",
    "        print(f\"   ‚úì Vocabulary size: {len(self.vocabulary)}\")\n",
    "    \n",
    "    def _create_tfidf_vectors(self):\n",
    "        \"\"\"Create TF-IDF vectors for clustering\"\"\"\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_features=1000,\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(self.combined_texts)\n",
    "        print(f\"   ‚úì TF-IDF matrix shape: {self.tfidf_matrix.shape}\")\n",
    "    \n",
    "    def _perform_clustering(self):\n",
    "        \"\"\"Perform K-Means clustering and store centroids\"\"\"\n",
    "        self.kmeans_model = KMeans(\n",
    "            n_clusters=self.n_clusters,\n",
    "            random_state=42,\n",
    "            n_init=10\n",
    "        )\n",
    "        self.doc_cluster_labels = self.kmeans_model.fit_predict(self.tfidf_matrix)\n",
    "        self.cluster_centroids = self.kmeans_model.cluster_centers_\n",
    "        \n",
    "        # Create cluster-to-documents mapping\n",
    "        self.cluster_to_docs = defaultdict(list)\n",
    "        for doc_idx, cluster_id in enumerate(self.doc_cluster_labels):\n",
    "            self.cluster_to_docs[cluster_id].append(doc_idx)\n",
    "        \n",
    "        print(f\"   ‚úì Clustering completed: {self.n_clusters} clusters\")\n",
    "        cluster_counts = defaultdict(int)\n",
    "        for label in self.doc_cluster_labels:\n",
    "            cluster_counts[label] += 1\n",
    "        print(f\"   ‚úì Cluster sizes: {dict(cluster_counts)}\")\n",
    "    \n",
    "    def _initialize_bm25_models(self):\n",
    "        \"\"\"Initialize three separate BM25 models for title, abstract, and keyphrases\"\"\"\n",
    "        self.bm25_title = BM25Okapi(self.processed_titles) if any(self.processed_titles) else None\n",
    "        self.bm25_abstract = BM25Okapi(self.processed_abstracts) if any(self.processed_abstracts) else None\n",
    "        self.bm25_keyphrases = BM25Okapi(self.processed_keyphrases) if any(self.processed_keyphrases) else None\n",
    "        \n",
    "        if self.bm25_title: print(\"   ‚úì BM25 model for title: initialized\")\n",
    "        if self.bm25_abstract: print(\"   ‚úì BM25 model for abstract: initialized\")\n",
    "        if self.bm25_keyphrases: print(\"   ‚úì BM25 model for keyphrases: initialized\")\n",
    "    \n",
    "    # ==================== Phase 2: Online Search ====================\n",
    "    \n",
    "    def search(self, query, top_n=10, k=60):\n",
    "        \"\"\"\n",
    "        Phase 2: Online Search\n",
    "        \n",
    "        Args:\n",
    "            query: Query string\n",
    "            top_n: Number of top results to return\n",
    "            k: RRF parameter\n",
    "        \n",
    "        Returns:\n",
    "            List of ranked documents\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üîç PHASE 2: ONLINE SEARCH\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\nüìå Query: '{query}'\")\n",
    "        \n",
    "        # Step 1: Typo correction\n",
    "        corrected_query = self._correct_typos(query)\n",
    "        \n",
    "        # Step 2: Preprocess query\n",
    "        query_tokens = preprocess(corrected_query)\n",
    "        print(f\"üìù Preprocessed query: {query_tokens}\")\n",
    "        \n",
    "        # Step 3: Filter clusters (find winning cluster)\n",
    "        winning_cluster = self._find_winning_cluster(corrected_query)\n",
    "        print(f\"üéØ Winning cluster: {winning_cluster}\")\n",
    "        print(f\"   Cluster contains {len(self.cluster_to_docs[winning_cluster])} documents\")\n",
    "        \n",
    "        # Step 4: Get sub-corpus (documents in winning cluster only)\n",
    "        sub_corpus_indices = self.cluster_to_docs[winning_cluster]\n",
    "        print(f\"üìö Searching within sub-corpus of {len(sub_corpus_indices)} documents\")\n",
    "        \n",
    "        # Step 5: BM25 ranking within the winning cluster\n",
    "        results = self._rank_within_cluster(query_tokens, sub_corpus_indices, k=k)\n",
    "        \n",
    "        # Step 6: Return top N results\n",
    "        print(f\"\\n‚úÖ Returning top {min(top_n, len(results))} results\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return results[:top_n]\n",
    "    \n",
    "    def _correct_typos(self, query):\n",
    "        \"\"\"\n",
    "        Step 1: Typo Correction using Levenshtein distance\n",
    "        \n",
    "        Args:\n",
    "            query: Original query\n",
    "        \n",
    "        Returns:\n",
    "            Corrected query\n",
    "        \"\"\"\n",
    "        query_tokens = preprocess(query)\n",
    "        corrected_tokens = []\n",
    "        corrections = []\n",
    "        \n",
    "        for token in query_tokens:\n",
    "            if token in self.vocabulary:\n",
    "                corrected_tokens.append(token)\n",
    "            else:\n",
    "                # Find closest word in vocabulary\n",
    "                min_dist = float('inf')\n",
    "                closest_word = token\n",
    "                \n",
    "                for vocab_word in self.vocabulary:\n",
    "                    dist = levenshtein_distance(token, vocab_word)\n",
    "                    if dist < min_dist:\n",
    "                        min_dist = dist\n",
    "                        closest_word = vocab_word\n",
    "                \n",
    "                # Only correct if distance <= 2\n",
    "                if min_dist <= 2:\n",
    "                    corrected_tokens.append(closest_word)\n",
    "                    if closest_word != token:\n",
    "                        corrections.append(f\"'{token}' -> '{closest_word}'\")\n",
    "                else:\n",
    "                    corrected_tokens.append(token)\n",
    "        \n",
    "        if corrections:\n",
    "            print(f\"üîß Typo corrections: {', '.join(corrections)}\")\n",
    "        \n",
    "        return ' '.join(corrected_tokens)\n",
    "    \n",
    "    def _find_winning_cluster(self, query):\n",
    "        \"\"\"\n",
    "        Step 3: Filter clusters using cosine similarity\n",
    "        \n",
    "        Args:\n",
    "            query: Preprocessed and corrected query\n",
    "        \n",
    "        Returns:\n",
    "            ID of the winning cluster\n",
    "        \"\"\"\n",
    "        # Transform query to TF-IDF vector\n",
    "        query_vector = self.tfidf_vectorizer.transform([query])\n",
    "        \n",
    "        # Calculate cosine similarity with all cluster centroids\n",
    "        similarities = cosine_similarity(query_vector, self.cluster_centroids)[0]\n",
    "        \n",
    "        # Find cluster with highest similarity\n",
    "        winning_cluster_id = np.argmax(similarities)\n",
    "        similarity_score = similarities[winning_cluster_id]\n",
    "        \n",
    "        print(f\"   Cluster {winning_cluster_id} has similarity: {similarity_score:.4f}\")\n",
    "        \n",
    "        return int(winning_cluster_id)\n",
    "    \n",
    "    def _rank_within_cluster(self, query_tokens, sub_corpus_indices, k=60):\n",
    "        \"\"\"\n",
    "        Step 5: BM25 Ranking within Winning Cluster\n",
    "        \n",
    "        Args:\n",
    "            query_tokens: Preprocessed query tokens\n",
    "            sub_corpus_indices: Document indices in the winning cluster\n",
    "            k: RRF parameter\n",
    "        \n",
    "        Returns:\n",
    "            List of ranked documents\n",
    "        \"\"\"\n",
    "        # Get rankings from each BM25 model\n",
    "        rank_maps = self._get_bm25_rankings(query_tokens, sub_corpus_indices)\n",
    "        \n",
    "        # Apply Reciprocal Rank Fusion (RRF)\n",
    "        rrf_scores = self._apply_rrf(rank_maps, k=k)\n",
    "        \n",
    "        # Sort by score and create result documents\n",
    "        sorted_scores = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        results = []\n",
    "        for doc_idx, score in sorted_scores:\n",
    "            doc = self.dataset[doc_idx]\n",
    "            results.append({\n",
    "                'id': doc['id'],\n",
    "                'title': doc.get('title', 'N/A'),\n",
    "                'score': score,\n",
    "                'abstract': doc.get('abstract', 'N/A'),\n",
    "                'keyphrases': doc.get('keyphrases', []),\n",
    "                'cluster_id': int(self.doc_cluster_labels[doc_idx])\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _get_bm25_rankings(self, query_tokens, sub_corpus_indices):\n",
    "        \"\"\"\n",
    "        Get rankings from all three BM25 models\n",
    "        \n",
    "        Args:\n",
    "            query_tokens: Query tokens\n",
    "            sub_corpus_indices: Document indices in the winning cluster\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of ranking maps\n",
    "        \"\"\"\n",
    "        rank_maps = {}\n",
    "        \n",
    "        if self.bm25_title:\n",
    "            # Get scores for all documents\n",
    "            all_title_scores = self.bm25_title.get_scores(query_tokens)\n",
    "            # Filter to sub-corpus only and rank\n",
    "            sub_title_scores = [(doc_idx, all_title_scores[doc_idx]) for doc_idx in sub_corpus_indices]\n",
    "            sub_title_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            rank_maps['title'] = {doc_idx: rank+1 \n",
    "                                   for rank, (doc_idx, _) in enumerate(sub_title_scores)}\n",
    "        \n",
    "        if self.bm25_abstract:\n",
    "            all_abstract_scores = self.bm25_abstract.get_scores(query_tokens)\n",
    "            sub_abstract_scores = [(doc_idx, all_abstract_scores[doc_idx]) for doc_idx in sub_corpus_indices]\n",
    "            sub_abstract_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            rank_maps['abstract'] = {doc_idx: rank+1 \n",
    "                                     for rank, (doc_idx, _) in enumerate(sub_abstract_scores)}\n",
    "        \n",
    "        if self.bm25_keyphrases:\n",
    "            all_keyphrase_scores = self.bm25_keyphrases.get_scores(query_tokens)\n",
    "            sub_keyphrase_scores = [(doc_idx, all_keyphrase_scores[doc_idx]) for doc_idx in sub_corpus_indices]\n",
    "            sub_keyphrase_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            rank_maps['keyphrases'] = {doc_idx: rank+1 \n",
    "                                       for rank, (doc_idx, _) in enumerate(sub_keyphrase_scores)}\n",
    "        \n",
    "        return rank_maps\n",
    "    \n",
    "    def _apply_rrf(self, rank_maps, k=60):\n",
    "        \"\"\"\n",
    "        Apply Reciprocal Rank Fusion (RRF)\n",
    "        \n",
    "        RRF Formula: RRF(d) = Œ£ 1/(k + rank_i(d))\n",
    "        \n",
    "        Args:\n",
    "            rank_maps: Dictionary of ranking maps\n",
    "            k: RRF parameter\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of document scores\n",
    "        \"\"\"\n",
    "        rrf_scores = defaultdict(float)\n",
    "        \n",
    "        all_docs = set()\n",
    "        for rank_map in rank_maps.values():\n",
    "            all_docs.update(rank_map.keys())\n",
    "        \n",
    "        for doc_idx in all_docs:\n",
    "            score = 0.0\n",
    "            for rank_map in rank_maps.values():\n",
    "                if doc_idx in rank_map:\n",
    "                    rank = rank_map[doc_idx]\n",
    "                    score += 1 / (k + rank)\n",
    "            rrf_scores[doc_idx] = score\n",
    "        \n",
    "        return rrf_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Cluster-then-Search Engine\n",
    "print(\"Initializing Cluster-then-Search Engine...\")\n",
    "engine = ClusterThenSearchEngine(dataset_to_eval, n_clusters=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Search 1: Computer Vision (with typo)\n",
    "query1 = \"computer visiom machne learing\"\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"EXAMPLE SEARCH 1\")\n",
    "print(f\"{'='*70}\")\n",
    "results1 = engine.search(query1, top_n=5, k=60)\n",
    "\n",
    "print(f\"\\nüìã Top 5 Results:\")\n",
    "for i, doc in enumerate(results1, 1):\n",
    "    print(f\"\\n{i}. Title: {doc['title']}\")\n",
    "    print(f\"   Score: {doc['score']:.6f}\")\n",
    "    print(f\"   Cluster ID: {doc['cluster_id']}\")\n",
    "    print(f\"   Abstract: {doc['abstract'][:100]}...\")\n",
    "    print(f\"   Keyphrases: {doc['keyphrases'][:3]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Search 2: Natural Language Processing\n",
    "query2 = \"natural language processing neural networks\"\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"EXAMPLE SEARCH 2\")\n",
    "print(f\"{'='*70}\")\n",
    "results2 = engine.search(query2, top_n=5, k=60)\n",
    "\n",
    "print(f\"\\nüìã Top 5 Results:\")\n",
    "for i, doc in enumerate(results2, 1):\n",
    "    print(f\"\\n{i}. Title: {doc['title']}\")\n",
    "    print(f\"   Score: {doc['score']:.6f}\")\n",
    "    print(f\"   Cluster ID: {doc['cluster_id']}\")\n",
    "    print(f\"   Abstract: {doc['abstract'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Search 3: Information Retrieval Systems\n",
    "query3 = \"information retrieval search engines\"\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"EXAMPLE SEARCH 3\")\n",
    "print(f\"{'='*70}\")\n",
    "results3 = engine.search(query3, top_n=5, k=60)\n",
    "\n",
    "print(f\"\\nüìã Top 5 Results:\")\n",
    "for i, doc in enumerate(results3, 1):\n",
    "    print(f\"\\n{i}. Title: {doc['title']}\")\n",
    "    print(f\"   Score: {doc['score']:.6f}\")\n",
    "    print(f\"   Cluster ID: {doc['cluster_id']}\")\n",
    "    print(f\"   Abstract: {doc['abstract'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dokumentasi Penggunaan Cluster-then-Search Engine\n",
    "\n",
    "## Ringkasan\n",
    "\n",
    "`ClusterThenSearchEngine` adalah search engine yang mengimplementasikan arsitektur Cluster-then-Search dengan dua fase utama:\n",
    "\n",
    "### **Fase 1: Offline Initialization**\n",
    "- ‚úÖ Loading dataset\n",
    "- ‚úÖ Preprocessing (tokenisasi, case folding, stemming)\n",
    "- ‚úÖ Vocabulary building untuk koreksi ejaan\n",
    "- ‚úÖ TF-IDF vectorization\n",
    "- ‚úÖ K-Means clustering (K=10)\n",
    "- ‚úÖ Inisialisasi 3 model BM25 (title, abstract, keyphrases)\n",
    "\n",
    "### **Fase 2: Online Search**\n",
    "- ‚úÖ **Typo Correction** dengan Levenshtein distance (threshold ‚â§ 2)\n",
    "- ‚úÖ **Cluster Filtering** menggunakan cosine similarity dengan cluster centroids\n",
    "- ‚úÖ **BM25 Ranking** HANYA pada dokumen di klaster yang menang\n",
    "- ‚úÖ **Reciprocal Rank Fusion (RRF)** untuk menggabungkan ranking dari 3 model BM25\n",
    "\n",
    "## Fitur Utama\n",
    "\n",
    "### 1. Typo Correction\n",
    "Menggunakan algoritma Levenshtein distance untuk mengoreksi typo dalam query. \n",
    "Hanya mengoreksi jika jarak edit ‚â§ 2.\n",
    "\n",
    "### 2. Cluster Filtering\n",
    "Menemukan klaster paling relevan untuk query menggunakan cosine similarity antara \n",
    "query vector (TF-IDF) dan cluster centroids.\n",
    "\n",
    "### 3. Efficient Search\n",
    "Hanya melakukan pencarian BM25 pada sub-korpus (dokumen dalam klaster yang menang), \n",
    "bukan pada seluruh korpus. Ini meningkatkan efisiensi dan relevansi hasil.\n",
    "\n",
    "### 4. Reciprocal Rank Fusion\n",
    "Menggabungkan 3 daftar ranking (dari title, abstract, keyphrases) menggunakan rumus:\n",
    "```\n",
    "RRF(d) = Œ£ 1/(k + rank_i(d))\n",
    "```\n",
    "dimana k = 60 (default) dan rank_i adalah ranking dokumen d di list ke-i.\n",
    "\n",
    "## Cara Menggunakan\n",
    "\n",
    "```python\n",
    "# 1. Initialize engine (Phase 1: Offline)\n",
    "engine = ClusterThenSearchEngine(dataset, n_clusters=10)\n",
    "\n",
    "# 2. Perform search (Phase 2: Online)\n",
    "results = engine.search(query=\"machine learning\", top_n=10, k=60)\n",
    "\n",
    "# 3. Access results\n",
    "for doc in results:\n",
    "    print(f\"Title: {doc['title']}\")\n",
    "    print(f\"Score: {doc['score']}\")\n",
    "    print(f\"Cluster ID: {doc['cluster_id']}\")\n",
    "    print(f\"Abstract: {doc['abstract']}\")\n",
    "```\n",
    "\n",
    "## Parameter\n",
    "\n",
    "- `n_clusters`: Jumlah klaster untuk K-Means (default: 10)\n",
    "- `top_n`: Jumlah hasil teratas yang ingin dikembalikan (default: 10)\n",
    "- `k`: Parameter RRF (default: 60)\n",
    "\n",
    "## Output\n",
    "\n",
    "Hasil pencarian berupa list dictionaries dengan keys:\n",
    "- `id`: Document ID\n",
    "- `title`: Judul dokumen\n",
    "- `score`: Skor RRF final\n",
    "- `abstract`: Abstract dokumen\n",
    "- `keyphrases`: List keyphrases\n",
    "- `cluster_id`: ID klaster dokumen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Multiple Queries - Comprehensive Example\n",
    "queries_to_test = [\n",
    "    \"deep learning neural networks\",\n",
    "    \"data mining clustering algorithms\",\n",
    "    \"recurrent neural networks\",\n",
    "    \"support vector machines\"\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"COMPREHENSIVE TEST: Multiple Queries\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for query in queries_to_test:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    results = engine.search(query, top_n=3, k=60)\n",
    "    \n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. {doc['title']}\")\n",
    "        print(f\"   Score: {doc['score']:.6f}\")\n",
    "        print(f\"   Cluster: {doc['cluster_id']}\")\n",
    "        print(f\"   Abstract preview: {doc['abstract'][:80]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Cluster Distribution\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"CLUSTER ANALYSIS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "from collections import Counter\n",
    "cluster_counts = Counter(engine.doc_cluster_labels)\n",
    "\n",
    "print(f\"\\nTotal documents: {engine.n_docs}\")\n",
    "print(f\"Number of clusters: {engine.n_clusters}\\n\")\n",
    "\n",
    "print(\"Cluster size distribution:\")\n",
    "for cluster_id in sorted(cluster_counts.keys()):\n",
    "    count = cluster_counts[cluster_id]\n",
    "    percentage = (count / engine.n_docs) * 100\n",
    "    print(f\"  Cluster {cluster_id}: {count:4d} documents ({percentage:5.1f}%)\")\n",
    "\n",
    "# Visualize cluster distribution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "ax1.bar(cluster_counts.keys(), cluster_counts.values(), color='skyblue', alpha=0.7)\n",
    "ax1.set_xlabel('Cluster ID')\n",
    "ax1.set_ylabel('Number of Documents')\n",
    "ax1.set_title('Cluster Distribution')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Pie chart\n",
    "ax2.pie(cluster_counts.values(), labels=[f'Cluster {i}' for i in cluster_counts.keys()], \n",
    "        autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('Cluster Size Proportions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary - Complete Cluster-then-Search Engine\n",
    "\n",
    "## Implementasi yang Telah Dibuat\n",
    "\n",
    "### **Phase 1: Offline Initialization** ‚úÖ\n",
    "1. Loading dataset dari Hugging Face\n",
    "2. Preprocessing terpisah (title, abstract, keyphrases)\n",
    "3. Vocabulary building untuk typo correction\n",
    "4. TF-IDF vectorization\n",
    "5. K-Means clustering dengan centroid storage\n",
    "6. 3 model BM25 terpisah\n",
    "\n",
    "### **Phase 2: Online Search** ‚úÖ\n",
    "1. **Typo Correction** - Levenshtein distance (threshold ‚â§ 2)\n",
    "2. **Cluster Filtering** - Cosine similarity dengan centroids\n",
    "3. **Efficient BM25** - Hanya pada sub-korpus (winning cluster)\n",
    "4. **Reciprocal Rank Fusion** - RRF(d) = Œ£ 1/(k + rank_i(d))\n",
    "5. **Top-N Results** - Dengan informasi lengkap\n",
    "\n",
    "## Fitur Utama\n",
    "\n",
    "- **Efficient**: Pencarian hanya pada winning cluster\n",
    "- **Robust**: Typo correction otomatis\n",
    "- **Accurate**: 3 model BM25 terpisah untuk multi-field ranking\n",
    "- **Flexible**: Parameter dapat disesuaikan (k, top_n, n_clusters)\n",
    "\n",
    "## Cara Penggunaan\n",
    "\n",
    "```python\n",
    "# Inisialisasi (sekali)\n",
    "engine = ClusterThenSearchEngine(dataset, n_clusters=10)\n",
    "\n",
    "# Pencarian (seberapa pun)\n",
    "results = engine.search(\"your query\", top_n=10, k=60)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
